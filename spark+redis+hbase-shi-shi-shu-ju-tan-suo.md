Druid是一个拥有大数据实时查询和分析的高容错、高性能开源分布式系统，旨在快速处理大规模的数据，并能够实现快速查询和分析。尤其是当发生代码部署、机器故障以及其他产品系统遇到宕机等情况时，Druid仍然能够保持100%正常运行。创建Druid的最初意图主要是为了解决查询延时问题，当时试图使用hadoop来实现交互式查询分析，但是很难满足实时分析的需要。而Druid提供了以交互方式访问数据的能力，并权衡了查询的灵活性和性能二采取了特殊的存储格式。

Druid允许以类似Dremel和PowerDrill的方式进行单表查询，同时还增加了一些新特性，如为局部嵌套数据结构提供列式存储格式、为快速过滤做索引、实时摄取和查询、高容错的分布式体系架构等。

特性

为分析而设计：为OLAP工作流的探索性分析而构建，支持各种过滤、聚合和查询等类；

快速的交互式查询：Druid的低延迟数据摄取架构允许事件在他们创建后毫秒内可被查询到；

高可用性：Druid的数据在系统更新时依然可用，规模的扩大和缩小都不会造成数据丢失；

可扩展：Druid已实现每天能够处理数十亿事件和TB级数据。

使用场景

1、需要交互式聚合和快速探究大量数据时；

2、需要实时查询分析时；

3、具有大量数据时，如每天数亿事件的新增、每天数10T数据的增加；

4、对数据尤其是大数据进行实时分析时；

5、需要一个高可用、高容错、高性能数据库时。

架构

Historical：对非实时数据进行处理存储和查询；

Realtime：实时摄取数据、监听输入数据流

Coordinator：监控historical节点

Broker：接收来自外部客户端的查询，和将查询转发到Realtime和historical

Indexer：负责索引服务

对比



Spark+Redis+Hbase 实时数据探索



代存在下述问题：



流量高峰期处理延迟



纬度交叉分析，不灵活



消耗资源大



系统故障，重算慢



这是第一代、消耗大、系统故障，在大内存情况下很容易导致崩溃。马蜂窝之前就遇到突发，一组三台，每一台 512 个 G，这个时候内存太大了，哪天一个内存条坏的话，这一天的数据可能就要重新算，而且对于现在当前整个实时数据量来看，完全就不符合当前的现状，算一天需要十几个小时。







当时考虑到，在数据量大的情况下，是不是我们可以去牺牲 UV 的计算。所以就引入在 Druid 里面。把 Druid 引入到 MES，误差基本上保持在 2% 左右。后面我们又通过雅虎提供的data sketch,可以精确调控 UV 的计算，它的默认值是 16384，16384 以下可以是精确的。当然这个值是可以控制的，就是 2 的 N 次幂，当前我们是调到特别大，800 多万。但 Druid 里面不支持MES第一代的虚拟 key。

